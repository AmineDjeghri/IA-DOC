{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Facial mapping / landmarks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1- Facial mapping (landmarks) with Dlib + python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lien du tutoriel: https://towardsdatascience.com/facial-mapping-landmarks-with-dlib-python-160abcf7d672"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### J'ai pu installer dlib sur windows 10 en utilisant pip install sur Spyder avec cette commande:\n",
    "\n",
    "#### `pip install https://files.pythonhosted.org/packages/0e/ce/f8a3cff33ac03a8219768f0694c5d703c8e037e6aba2e865f9bae22ed63c/dlib-19.8.1-cp36-cp36m-win_amd64.whl#sha256=794994fa2c54e7776659fddb148363a5556468a6d5d46be8dad311722d54bfcf`\n",
    "\n",
    "#### `pip install dlib` buggé sur Spyder quand j'exécutais cette commande"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-60220a8665b0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[1;31m# Getting our image by webcam and converting it into a gray image scale\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcap\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0mgray\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCOLOR_BGR2GRAY\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "\n",
    "# if (you have only 1 webcam){ set device = 0} else{ chose your favorite webcam setting device = 1, 2 ,3 ... }\n",
    "cap=cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    # Getting our image by webcam and converting it into a gray image scale\n",
    "    _,image=cap.read()\n",
    "    gray=cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    #show the grey image\n",
    "    cv2.imshow(\"Output\",image)\n",
    "    \n",
    "    #key to give up the app (it is 'q')\n",
    "    k=cv2.waitKey(5) & 0xFF\n",
    "    if k ==ord('q'):\n",
    "        break;\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "cap.release()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's go code an faces detector(HOG) and after detect the landmarks on this detected face"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-9ecaeda29035>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[1;31m# Get faces into webcam's image\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m     \u001b[0mrects\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdetector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgray\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[1;31m# For each detected face, find the landmark.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "from imutils import face_utils\n",
    "import dlib\n",
    "import cv2\n",
    "\n",
    "# p = our pre-treined model directory, on my case, it's on the same script's diretory.\n",
    "p = \"shape_predictor_68_face_landmarks.dat\"\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(p)\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    # Getting out image by webcam \n",
    "    _, image = cap.read()\n",
    "    # Converting the image to gray scale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "    # Get faces into webcam's image\n",
    "    rects = detector(gray, 0)    \n",
    "    \n",
    "    # For each detected face, find the landmark.\n",
    "    for (i, rect) in enumerate(rects):\n",
    "        # Make the prediction and transfom it to numpy array\n",
    "        shape = predictor(gray, rect)\n",
    "        shape = face_utils.shape_to_np(shape)\n",
    "    \n",
    "        # Draw on our image, all the finded cordinate points (x,y) \n",
    "        for (x, y) in shape:\n",
    "            cv2.circle(image, (x, y), 2, (0, 255, 0), -1)\n",
    "    \n",
    "    # Show the image\n",
    "    cv2.imshow(\"Output\", image)\n",
    "    \n",
    "    k = cv2.waitKey(5) & 0xFF\n",
    "    if k == ord('q'):\n",
    "        break\n",
    "\n",
    "cv2.destroyAllWindows()\n",
    "cap.release()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2-Facial landmarks with dlib, OpenCV, and Python\n",
    "\n",
    "### (tutoriel plus approfondi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lien du tutoriel: \n",
    "https://www.pyimagesearch.com/2017/04/03/facial-landmarks-dlib-opencv-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "from collections import OrderedDict\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "# define a dictionary that maps the indexes of the facial\n",
    "# landmarks to specific face regions\n",
    "FACIAL_LANDMARKS_IDXS = OrderedDict([\n",
    "    (\"mouth\", (48, 68)),\n",
    "    (\"right_eyebrow\", (17, 22)),\n",
    "    (\"left_eyebrow\", (22, 27)),\n",
    "    (\"right_eye\", (36, 42)),\n",
    "    (\"left_eye\", (42, 48)),\n",
    "    (\"nose\", (27, 36)),\n",
    "    (\"jaw\", (0, 17))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1- fonction pour définir le cadre de selection (bounding box) juste pour nous aider pour OpenCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rect_to_bb(rect):\n",
    "    # take a bounding predicted by dlib and convert it\n",
    "    # to the format (x, y, w, h) as we would normally do\n",
    "    # with OpenCV\n",
    "    x = rect.left()\n",
    "    y = rect.top()\n",
    "    w = rect.right() - x\n",
    "    h = rect.bottom() - y\n",
    " \n",
    "    # return a tuple of (x, y, w, h)\n",
    "    return (x, y, w, h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2-convert this object to a NumPy array, allowing it to “play nicer” with our Python code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shape_to_np(shape, dtype=\"int\"):\n",
    "    # initialize the list of (x, y)-coordinates\n",
    "    coords = np.zeros((68, 2), dtype=dtype)\n",
    "\n",
    "    # loop over the 68 facial landmarks and convert them\n",
    "    # to a 2-tuple of (x, y)-coordinates\n",
    "    for i in range(0, 68):\n",
    "        coords[i] = (shape.part(i).x, shape.part(i).y)\n",
    "\n",
    "    # return the list of (x, y)-coordinates\n",
    "    return coords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Given these two helper functions, we are now ready to detect facial landmarks in images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "writing code here or inside a new file (facial_landmarks.py)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# initialize dlib's face detector (HOG-based) and then create\n",
    "# the facial landmark predictor\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor('shape_predictor_68_face_landmarks.dat')\n",
    "\n",
    "# load the input image, resize it, and convert it to grayscale\n",
    "\n",
    "\n",
    "#image = cv2.imread('img1.jpg')\n",
    "image= cv2.imread('img2.jpg')\n",
    "\n",
    "image = imutils.resize(image, width=500)\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# detect faces in the grayscale image\n",
    "rects = detector(gray, 1)\n",
    "\n",
    "# loop over the face detections\n",
    "for (i, rect) in enumerate(rects):\n",
    "    # determine the facial landmarks for the face region, then\n",
    "    # convert the facial landmark (x, y)-coordinates to a NumPy\n",
    "    # array\n",
    "    shape = predictor(gray, rect)\n",
    "    shape = face_utils.shape_to_np(shape)\n",
    "\n",
    "    # convert dlib's rectangle to a OpenCV-style bounding box\n",
    "    # [i.e., (x, y, w, h)], then draw the face bounding box\n",
    "    (x, y, w, h) = face_utils.rect_to_bb(rect)\n",
    "    cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "    # show the face number\n",
    "    cv2.putText(image, \"Face #{}\".format(i + 1), (x - 10, y - 10),\n",
    "        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "    # loop over the (x, y)-coordinates for the facial landmarks\n",
    "    # and draw them on the image\n",
    "    for (x, y) in shape:\n",
    "        cv2.circle(image, (x, y), 1, (0, 0, 255), -1)\n",
    "\n",
    "# show the output image with the face detections + facial landmarks\n",
    "cv2.imshow(\"Output\", image)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # 3- 2nd tutorial + face part detection "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from cv2 import *\n",
    "from imutils import face_utils\n",
    "import numpy as np\n",
    "import argparse\n",
    "import imutils\n",
    "import dlib\n",
    "import os\n",
    "import glob\n",
    "from os.path import basename, splitext\n",
    "\n",
    "\n",
    "\n",
    "# initialize dlib's face detector (HOG-based) and then create\n",
    "# the facial landmark predictor\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(\"shape_predictor_68_face_landmarks.dat\")\n",
    "\n",
    "# load the input image, resize it, and convert it to grayscale\n",
    "#image = cv2.imread(data_path + '/'+ dataset + '/'+ img)\n",
    "image= cv2.imread('img1.jpg')\n",
    "image = imutils.resize(image, width=500)\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    # detect faces in the grayscale image\n",
    "rects = detector(gray, 1)\n",
    "\n",
    "    # loop over the face detections\n",
    "for (i, rect) in enumerate(rects):\n",
    "        # determine the facial landmarks for the face region, then\n",
    "        # convert the landmark (x, y)-coordinates to a NumPy array\n",
    "        shape = predictor(gray, rect)\n",
    "        shape = face_utils.shape_to_np(shape)\n",
    "\n",
    "        # loop over the face parts individually\n",
    "        for (name, (i, j)) in face_utils.FACIAL_LANDMARKS_IDXS.items():\n",
    "            # clone the original image so we can draw on it, then\n",
    "            # display the name of the face part on the image\n",
    "            clone = image.copy()\n",
    "            cv2.putText(clone, name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                0.7, (0, 0, 255), 2)\n",
    "\n",
    "            # loop over the subset of facial landmarks, drawing the\n",
    "            # specific face part\n",
    "            for (x, y) in shape[i:j]:\n",
    "                cv2.circle(clone, (x, y), 1, (0, 0, 255), -1)\n",
    "\n",
    "            # extract the ROI of the face region as a separate image\n",
    "            (x, y, w, h) = cv2.boundingRect(np.array([shape[i:j]]))\n",
    "            roi = image[y:y + h, x:x + w]\n",
    "            try:\n",
    "                roi = imutils.resize(roi, width=250, inter=cv2.INTER_CUBIC)\n",
    "            except ZeroDivisionError:\n",
    "                print(\"Plusieurs\")\n",
    "\n",
    "\n",
    "            #Add saving on disk\n",
    "            #see part4\n",
    "\n",
    "            cv2.imshow(\"Output\", clone)\n",
    "            #cv2.imshow(\"Output\", roi)\n",
    "\n",
    "            k=cv2.waitKey(0)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4-  3rd tutorial + saving on the disk face parts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize dlib's face detector (HOG-based) and then create\n",
    "# the facial landmark predictor\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor('shape_predictor_68_face_landmarks.dat')\n",
    "\n",
    "# load the input image, resize it, and convert it to grayscale\n",
    "    \n",
    "#image = cv2.imread('img1.jpg')\n",
    "image= cv2.imread('dataset/img2.jpg')\n",
    "\n",
    "image = imutils.resize(image, width=500)\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# detect faces in the grayscale image\n",
    "rects = detector(gray, 1)\n",
    "\n",
    "# loop over the face detections\n",
    "for (i, rect) in enumerate(rects):\n",
    "    # determine the facial landmarks for the face region, then\n",
    "    # convert the facial landmark (x, y)-coordinates to a NumPy\n",
    "    # array\n",
    "    shape = predictor(gray, rect)\n",
    "    shape = face_utils.shape_to_np(shape)\n",
    "\n",
    "    # convert dlib's rectangle to a OpenCV-style bounding box\n",
    "    # [i.e., (x, y, w, h)], then draw the face bounding box\n",
    "    (x, y, w, h) = face_utils.rect_to_bb(rect)\n",
    "    cv2.rectangle(image, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "    # show the face number\n",
    "    cv2.putText(image, \"Face #{}\".format(i + 1), (x - 10, y - 10),\n",
    "        cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "    # loop over the face parts individually\n",
    "    for (name, (i, j)) in face_utils.FACIAL_LANDMARKS_IDXS.items():\n",
    "        # clone the original image so we can draw on it, then\n",
    "        # display the name of the face part on the image\n",
    "        clone = image.copy()\n",
    "        cv2.putText(clone, name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            0.7, (0, 0, 255), 2)\n",
    "\n",
    "        # loop over the subset of facial landmarks, drawing the\n",
    "        # specific face part\n",
    "        for (x, y) in shape[i:j]:\n",
    "            cv2.circle(clone, (x, y), 1, (0, 0, 255), -1)\n",
    "\n",
    "        # extract the ROI of the face region as a separate image\n",
    "        (x, y, w, h) = cv2.boundingRect(np.array([shape[i:j]]))\n",
    "        roi = image[y:y + h, x:x + w]\n",
    "        try:\n",
    "            roi = imutils.resize(roi, width=250, inter=cv2.INTER_CUBIC)\n",
    "        except ZeroDivisionError:\n",
    "            print(\"Plusieurs\")\n",
    "            \n",
    "        # show the particular face part\n",
    "        if not os.path.exists(\"landmarks/\"):\n",
    "                os.makedirs(\"landmarks/\")\n",
    "\n",
    "        cv2.imwrite(\"landmarks/\" + name + \".jpg\", roi)\n",
    "  \n",
    "        cv2.imshow(\"Output\", roi)\n",
    "        cv2.waitKey(0)\n",
    "\n",
    "# show the output image with the face detections + facial landmarks\n",
    "cv2.imshow(\"Output\", image)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5-  \n",
    "\n",
    "4th one + searching all the images in the folders, and devide landmarks into pictures and save them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['dataset1', 'dataset2']\n",
      "dataset1\n",
      "Loaded the images of dataset-dataset1\n",
      " ['img1.jpg', 'img2.jpg']\n",
      "img1.jpg\n",
      "img2.jpg\n",
      "dataset2\n",
      "Loaded the images of dataset-dataset2\n",
      " ['img3.jpg', 'img4.jpg']\n",
      "img3.jpg\n",
      "img4.jpg\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "from cv2 import *\n",
    "from imutils import face_utils\n",
    "import numpy as np\n",
    "import argparse\n",
    "import imutils\n",
    "import dlib\n",
    "import os\n",
    "import glob\n",
    "from os.path import basename, splitext\n",
    "\n",
    "\n",
    "# initialize dlib's face detector (HOG-based) and then create\n",
    "# the facial landmark predictor\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor('shape_predictor_68_face_landmarks.dat')\n",
    "\n",
    "# load the input image, resize it, and convert it to grayscale\n",
    "\n",
    "\n",
    "data_path = os.getcwd()+\"/dataset\"\n",
    "data_dir_list = os.listdir(data_path)\n",
    "\n",
    "print(data_dir_list)\n",
    "\n",
    "for dataset in data_dir_list:\n",
    "    print(dataset)\n",
    "    img_list=os.listdir(data_path+'/'+ dataset+'/')\n",
    "    \n",
    "    print ('Loaded the images of dataset-'+'{}\\n'.format(dataset), img_list)\n",
    "    \n",
    "    \n",
    "    for img in img_list:\n",
    "        print(img)\n",
    "    \n",
    "        #image = cv2.imread('img1.jpg')\n",
    "        image= cv2.imread(data_path+\"/\"+dataset+\"/\"+img)\n",
    "\n",
    "        image = imutils.resize(image, width=500)\n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # detect faces in the grayscale image\n",
    "        rects = detector(gray, 1)\n",
    "        \n",
    "        # clone the original image so we can draw on it, and save the face parts without the draws\n",
    "        clone = image.copy()\n",
    "\n",
    "        # loop over the face detections\n",
    "        for (i, rect) in enumerate(rects):\n",
    "            # determine the facial landmarks for the face region, then\n",
    "            # convert the facial landmark (x, y)-coordinates to a NumPy\n",
    "            # array\n",
    "            shape = predictor(gray, rect)\n",
    "            shape = face_utils.shape_to_np(shape)\n",
    "\n",
    "            # convert dlib's rectangle to a OpenCV-style bounding box\n",
    "            # [i.e., (x, y, w, h)], then draw the face bounding box\n",
    "            (x, y, w, h) = face_utils.rect_to_bb(rect)\n",
    "            \n",
    "            cv2.rectangle(clone, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "            # show the face number\n",
    "            cv2.putText(image, \"Face #{}\".format(i + 1), (x - 10, y - 10),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 0), 2)\n",
    "\n",
    "            # loop over the face parts individually\n",
    "            for (name, (k, j)) in face_utils.FACIAL_LANDMARKS_IDXS.items():\n",
    "                # display the name of the face part on the image\n",
    "                cv2.putText(clone, name, (10, 30), cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    0.7, (0, 0, 255), 2)\n",
    "\n",
    "                # loop over the subset of facial landmarks, drawing the\n",
    "                # specific face part\n",
    "                for (x, y) in shape[k:j]:\n",
    "                    cv2.circle(clone, (x, y), 1, (0, 0, 255), -1)\n",
    "\n",
    "                # extract the ROI of the face region as a separate image\n",
    "                (x, y, w, h) = cv2.boundingRect(np.array([shape[k:j]]))\n",
    "                roi = image[y:y + h, x:x + w]\n",
    "                try:\n",
    "                    roi = imutils.resize(roi, width=250, inter=cv2.INTER_CUBIC)\n",
    "                except ZeroDivisionError:\n",
    "                    print(\"Plusieurs\")\n",
    "\n",
    "                # show the particular face part\n",
    "                if not os.path.exists(\"landmarks/\"):\n",
    "                        os.makedirs(\"landmarks/\")\n",
    "                \n",
    "                if not os.path.exists(\"landmarks/\"+dataset+\"/\"+name+\"/\"):\n",
    "                        os.makedirs(\"landmarks/\"+dataset+\"/\"+name+\"/\")\n",
    "                  \n",
    "                #name because of multiple faces in one image\n",
    "                img_name=str(i)+img        \n",
    "                cv2.imwrite(\"landmarks/\" +dataset+\"/\"+ name+\"/\"+img_name , roi)\n",
    "\n",
    "                \n",
    "        # show the output image with the face detections + facial landmarks\n",
    "        #cv2.imshow(\"Output\", clone)\n",
    "        #cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6- 5th one but on csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
